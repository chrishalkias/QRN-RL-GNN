network = <repeaters.RepeaterNetwork object at 0x10f5fdd30> 
n = 4 
lr = 0.0003 
gamma = 0.95 
epsilon = 0.2 
criterion = CrossEntropyLoss() 
weight_decay = 1e-05 
memory = [] 
model = GNN(
  (encoder): Sequential(
    (0): GATConv(1, 4, heads=2)
    (1): GATConv(8, 4, heads=2)
  )
  (latent): Sequential(
    (0): Linear(in_features=8, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=64, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=64, out_features=4, bias=True)
    (1): Softmax(dim=-1)
  )
) 
temperature = 1 
optimizer = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 1e-05
) 
scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x10f5fd940> 

 Model breakdown 
+-----------------------+----------------+----------------+----------+
| Layer                 | Input Shape    | Output Shape   | #Param   |
|-----------------------+----------------+----------------+----------|
| GNN                   | [4, 4]         | [4, 4]         | 2,780    |
| ├─(encoder)Sequential | --             | --             | 120      |
| │    └─(0)GATConv     | [4, 1], [2, 3] | [4, 8]         | 32       |
| │    └─(1)GATConv     | [4, 8], [2, 3] | [4, 8]         | 88       |
| ├─(latent)Sequential  | [4, 8]         | [4, 64]        | 2,400    |
| │    └─(0)Linear      | [4, 8]         | [4, 32]        | 288      |
| │    └─(1)ReLU        | [4, 32]        | [4, 32]        | --       |
| │    └─(2)Linear      | [4, 32]        | [4, 64]        | 2,112    |
| ├─(decoder)Sequential | [4, 64]        | [4, 4]         | 260      |
| │    └─(0)Linear      | [4, 64]        | [4, 4]         | 260      |
| │    └─(1)Softmax     | [4, 4]         | [4, 4]         | --       |
+-----------------------+----------------+----------------+----------+
Total params: 2,780

-Training information (Q-learning) 
 Trained for 2.241 sec performing 1000 steps.
trained, L=12, t_avg=77.4, t_std=69.7
random, L=8, t_avg=117.0, t_std=83.7
swapASAP, L=165, t_avg=6.0, t_std=8.0
alternating, L=220, t_avg=4.5, t_std=3.6
