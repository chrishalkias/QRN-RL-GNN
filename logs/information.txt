network = <repeaters.RepeaterNetwork object at 0x10a80dd30> 
n = 4 
lr = 0.0003 
gamma = 0.9 
epsilon = 0.1 
criterion = MSELoss() 
weight_decay = 1e-05 
memory = [] 
model = GNN(
  (encoder): Sequential(
    (0): GATConv(1, 4, heads=2)
    (1): GATConv(8, 4, heads=2)
  )
  (latent): Sequential(
    (0): Linear(in_features=8, out_features=32, bias=True)
    (1): ReLU()
    (2): Linear(in_features=32, out_features=64, bias=True)
  )
  (decoder): Sequential(
    (0): Linear(in_features=64, out_features=4, bias=True)
    (1): Softmax(dim=-1)
  )
) 
temperature = 1 
optimizer = Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0003
    maximize: False
    weight_decay: 1e-05
) 
scheduler = <torch.optim.lr_scheduler.ReduceLROnPlateau object at 0x10a80d940> 

 Model breakdown 
+-----------------------+----------------+----------------+----------+
| Layer                 | Input Shape    | Output Shape   | #Param   |
|-----------------------+----------------+----------------+----------|
| GNN                   | [4, 4]         | [4, 4]         | 2,780    |
| ├─(encoder)Sequential | --             | --             | 120      |
| │    └─(0)GATConv     | [4, 1], [2, 3] | [4, 8]         | 32       |
| │    └─(1)GATConv     | [4, 8], [2, 3] | [4, 8]         | 88       |
| ├─(latent)Sequential  | [4, 8]         | [4, 64]        | 2,400    |
| │    └─(0)Linear      | [4, 8]         | [4, 32]        | 288      |
| │    └─(1)ReLU        | [4, 32]        | [4, 32]        | --       |
| │    └─(2)Linear      | [4, 32]        | [4, 64]        | 2,112    |
| ├─(decoder)Sequential | [4, 64]        | [4, 4]         | 260      |
| │    └─(0)Linear      | [4, 64]        | [4, 4]         | 260      |
| │    └─(1)Softmax     | [4, 4]         | [4, 4]         | --       |
+-----------------------+----------------+----------------+----------+
Total params: 2,780

            Training information (Q-learning) 
 Trained for 186.892 sec performing 80000 steps.
trained, L=62, t_avg=64.5, t_std=68.6
random, L=30, t_avg=132.7, t_std=117.2
alternating, L=860, t_avg=4.7, t_std=3.3
