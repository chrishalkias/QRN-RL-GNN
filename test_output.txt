Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 10 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 12 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 14 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 16 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 18 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 20 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 22 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 24 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 26 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 28 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 30 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 32 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 34 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 36 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 38 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 40 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 42 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 44 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 46 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 48 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 50 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 52 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 54 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 56 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 58 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 60 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 62 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 64 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 66 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 68 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 70 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 72 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 74 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 76 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 78 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 80 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 82 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 84 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 86 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 88 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 90 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 92 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 94 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 96 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 98 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 129 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 387 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 507 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 581 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 611 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1055 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1159 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1257 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1403 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1533 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1655 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1715 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1765 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1851 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 1977 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 1998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 2123 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 2709 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 2851 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 2939 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 2998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3135 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3345 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3363 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3515 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3601 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3617 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3651 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 3755 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 3998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 4097 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 4209 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 4495 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 4631 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 4959 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 4998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 5049 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 5573 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 5753 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 5783 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 5851 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 5998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 6369 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 6511 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 6617 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 6998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 7005 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 7437 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 7739 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 7777 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 7925 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 7998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8010 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8012 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8093 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8281 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8347 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8554 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8556 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8590 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8592 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8627 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8669 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8780 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8782 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 8977 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 8998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9000 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9002 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9004 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9006 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9008 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 9011 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9014 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9016 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9018 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9020 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9022 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9024 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9026 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9028 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9030 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9032 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9034 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9036 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9038 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9040 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9042 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9044 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9046 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9048 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9050 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9052 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9054 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9056 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9058 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9060 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9062 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9064 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9066 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9068 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9070 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9072 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9074 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9076 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9078 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9080 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9082 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9084 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9086 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9088 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9090 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9092 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9094 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9096 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9098 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9100 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9102 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9104 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9106 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9108 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9110 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9112 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9114 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9116 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9118 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9120 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9122 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9124 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9126 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9128 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9130 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9132 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9134 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9136 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9138 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9140 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9142 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9144 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9146 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9148 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9150 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9152 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9154 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9156 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9158 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9160 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9162 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9164 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9166 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9168 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9170 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9172 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9174 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9176 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9178 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9180 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9182 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9184 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9186 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9188 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9190 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9192 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9194 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9196 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9198 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9200 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9202 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9204 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9206 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9208 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9210 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9212 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9214 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9216 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9218 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9220 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9222 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9224 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9226 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9228 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9230 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9232 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9234 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9236 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9238 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9240 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9242 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9244 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9246 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9248 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9250 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9252 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9254 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9256 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9258 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9260 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9262 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9264 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9266 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9268 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9270 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9272 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9274 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9276 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9278 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9280 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9282 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9284 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9286 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9288 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9290 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9292 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9294 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9296 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9298 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9300 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9302 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9304 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9306 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9308 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9310 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9312 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9314 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9316 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9318 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9320 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9322 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9324 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9326 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9328 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9330 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9332 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9334 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9336 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9338 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9340 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9342 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9344 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9346 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9348 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9350 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9352 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9354 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9356 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9358 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9360 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9362 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9364 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9366 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9368 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9370 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9372 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9374 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9376 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9378 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9380 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9382 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9384 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9386 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9388 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9390 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9392 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9394 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9396 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9398 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9400 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9402 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9404 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9406 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9408 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9410 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9412 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9414 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9416 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9418 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9420 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9422 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9424 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9426 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9428 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9430 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9432 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9434 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9436 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9438 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9440 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9442 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9444 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9446 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9448 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9450 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9452 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9454 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9456 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9458 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9460 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9462 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9464 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9466 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9468 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9470 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9472 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9474 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9476 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9478 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9480 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9482 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9484 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9486 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9488 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9490 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9492 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9494 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9496 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9498 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9500 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9502 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9504 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9506 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9508 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9510 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9512 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9514 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9516 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9518 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9520 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9522 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9524 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9526 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9528 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9530 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9532 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9534 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9536 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9538 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9540 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9542 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9544 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9546 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9548 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9550 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9552 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 9555 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9558 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9560 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9562 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9564 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9566 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9568 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9570 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9572 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9574 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9576 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9578 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9580 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9582 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9584 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9586 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9588 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 9591 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9594 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9596 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9598 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9600 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9602 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9604 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9606 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9608 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9610 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9612 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9614 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9616 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9618 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9620 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9622 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9624 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9626 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9628 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9630 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9632 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9634 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9636 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9638 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9640 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9642 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9644 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9646 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9648 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9650 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9652 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9654 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9656 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9658 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9660 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9662 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9664 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9666 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9668 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9670 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9672 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9674 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9676 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9678 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9680 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9682 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9684 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9686 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9688 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9690 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9692 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9694 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9696 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9698 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9700 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9702 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9704 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9706 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9708 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9710 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9712 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9714 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9716 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9718 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9720 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9722 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9724 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9726 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9728 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9730 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9732 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9734 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9736 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9738 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9740 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9742 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9744 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9746 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9748 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9750 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9752 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9754 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9756 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9758 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9760 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9762 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9764 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9766 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9768 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9770 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9772 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9774 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9776 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9778 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: 1End-to-end entanglement achieved in 9781 steps
Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: -0.1Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9784 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9786 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9788 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9790 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9792 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9794 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9796 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9798 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9800 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9802 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9804 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9806 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9808 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9810 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9812 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9814 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9816 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9818 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9820 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9822 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9824 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9826 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9828 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9830 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9832 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9834 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9836 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9838 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9840 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9842 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9844 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9846 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9848 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9850 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9852 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9854 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9856 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9858 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9860 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9862 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9864 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9866 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9868 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9870 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9872 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9874 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9876 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9878 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9880 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9882 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9884 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9886 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9888 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9890 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9892 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9894 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9896 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9898 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9900 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9902 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9904 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9906 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9908 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9910 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9912 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9914 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9916 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9918 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9920 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9922 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9924 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9926 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9928 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9930 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9932 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9934 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9936 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9938 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9940 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9942 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9944 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9946 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9948 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9950 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9952 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9954 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9956 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9958 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9960 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9962 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9964 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9966 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9968 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9970 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9972 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9974 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9976 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9978 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9980 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9982 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9984 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9986 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9988 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9990 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9992 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9994 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9996 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 9998 steps
Action: ['self.entangle((0, 1))', 'self.entangle((1, 2))', 'self.entangle((2, 3))', 'self.entangle((3, 4))'],Reward: -0.1Action: ['self.swapAT(0)', 'self.swapAT(1)', 'self.swapAT(2)', 'self.swapAT(3)', 'self.swapAT(4)'],Reward: 1End-to-end entanglement achieved in 10000 steps
Max iterations reached
